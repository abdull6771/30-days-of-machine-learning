{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 Days of Machine Learning\n",
    "\n",
    "Welcome to the **30 Days of Machine Learning** project! This Jupyter Notebook guides you through 30 days of hands-on machine learning projects, from beginner to intermediate levels. Each day focuses on a specific concept, technique, or project, building your skills progressively.\n",
    "\n",
    "## How to Use This Notebook\n",
    "- **Daily Tasks**: Each day has a markdown cell with objectives, resources, and steps, followed by code cells for implementation.\n",
    "- **Code Cells**: All days now include complete code examples.\n",
    "- **Resources**: Links to datasets (e.g., Kaggle) and tutorials are provided.\n",
    "- **Progress Tracking**: Run and modify code cells as you complete each task. Save your work regularly.\n",
    "- **Sharing**: On Day 30, share your projects on GitHub or Kaggle.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.x\n",
    "- Libraries: NumPy, Pandas, Scikit-learn, TensorFlow/Keras, Matplotlib, Seaborn, Flask, NLTK, Rasa, XGBoost\n",
    "- Install dependencies: `pip install numpy pandas scikit-learn tensorflow matplotlib seaborn flask nltk rasa xgboost`\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1: Introduction to Python for ML\n",
    "**Objective**: Set up your environment and learn NumPy and Pandas basics.\n",
    "**Resources**: [NumPy Docs](https://numpy.org/doc/stable/), [Pandas Docs](https://pandas.pydata.org/docs/)\n",
    "**Steps**:\n",
    "1. Install Python, Jupyter, and required libraries.\n",
    "2. Create NumPy arrays and perform basic operations.\n",
    "3. Load a CSV file with Pandas and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"2D Array:\\n\", array)\n",
    "\n",
    "# Perform basic operations\n",
    "array_sum = np.sum(array)\n",
    "array_mean = np.mean(array)\n",
    "print(\"Sum:\", array_sum)\n",
    "print(\"Mean:\", array_mean)\n",
    "\n",
    "# Create a sample DataFrame (simulating a CSV)\n",
    "data = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'London', 'Paris']\n",
    "})\n",
    "print(\"\\nFirst 5 rows of DataFrame:\\n\", data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2: Exploratory Data Analysis (EDA)\n",
    "**Objective**: Perform EDA on a Kaggle dataset.\n",
    "**Dataset**: [Titanic](https://www.kaggle.com/c/titanic/data)\n",
    "**Steps**:\n",
    "1. Load the dataset with Pandas.\n",
    "2. Visualize distributions with Matplotlib/Seaborn.\n",
    "3. Identify missing values and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Titanic dataset (download from Kaggle or use sample data)\n",
    "# For this example, we'll create a simplified dataset\n",
    "data = pd.DataFrame({\n",
    "    'Survived': [0, 1, 1, 0, 1],\n",
    "    'Pclass': [3, 1, 2, 3, 1],\n",
    "    'Age': [22, 38, 26, np.nan, 35],\n",
    "    'Fare': [7.25, 71.83, 13.0, 8.05, 53.1]\n",
    "})\n",
    "\n",
    "# Summarize missing values\n",
    "print(\"Missing Values:\\n\", data.isnull().sum())\n",
    "\n",
    "# Plot histograms\n",
    "data['Age'].hist(bins=10)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3: Linear Regression\n",
    "**Objective**: Build a linear regression model to predict house prices.\n",
    "**Dataset**: [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
    "**Steps**:\n",
    "1. Load and preprocess the dataset.\n",
    "2. Train a linear regression model.\n",
    "3. Evaluate with RMSE and visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset (replace with actual path)\n",
    "# For this example, we'll simulate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10  # Square footage\n",
    "y = 50 + 30 * X + np.random.randn(100, 1) * 10  # Price\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4: Data Preprocessing\n",
    "**Objective**: Handle missing values and encode categorical data.\n",
    "**Dataset**: Titanic or any dataset with missing values\n",
    "**Steps**:\n",
    "1. Impute missing numerical values (mean/median).\n",
    "2. Encode categorical variables (One-Hot or Label Encoding).\n",
    "3. Scale numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Create sample dataset (simulating Titanic)\n",
    "data = pd.DataFrame({\n",
    "    'Age': [22, 38, np.nan, 35],\n",
    "    'Sex': ['male', 'female', 'male', 'female'],\n",
    "    'Fare': [7.25, 71.83, 8.05, 53.1]\n",
    "})\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['Age'] = imputer.fit_transform(data[['Age']])\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "sex_encoded = encoder.fit_transform(data[['Sex']])\n",
    "data['Sex_male'] = sex_encoded[:, 0]\n",
    "data = data.drop('Sex', axis=1)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['Age', 'Fare']] = scaler.fit_transform(data[['Age', 'Fare']])\n",
    "\n",
    "print(\"Preprocessed Data:\\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5: Logistic Regression\n",
    "**Objective**: Build a binary classification model.\n",
    "**Dataset**: [Iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
    "**Steps**:\n",
    "1. Load Iris dataset (binary subset).\n",
    "2. Train logistic regression model.\n",
    "3. Evaluate with accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset (binary subset: Setosa vs. Versicolor)\n",
    "iris = load_iris()\n",
    "X = iris.data[iris.target != 2][:, :2]  # Use first two features\n",
    "y = iris.target[iris.target != 2]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6: Decision Trees\n",
    "**Objective**: Use decision trees for classification.\n",
    "**Dataset**: [Wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html)\n",
    "**Steps**:\n",
    "1. Load Wine dataset.\n",
    "2. Train a decision tree classifier.\n",
    "3. Visualize the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train decision tree\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Visualize tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, feature_names=wine.feature_names, class_names=wine.target_names, filled=True)\n",
    "plt.title('Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 7: Random Forest\n",
    "**Objective**: Improve classification with Random Forest.\n",
    "**Dataset**: Wine or Titanic\n",
    "**Steps**:\n",
    "1. Train a Random Forest classifier.\n",
    "2. Compare performance with decision tree.\n",
    "3. Analyze feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f'Decision Tree Accuracy: {dt_accuracy:.2f}')\n",
    "print(f'Random Forest Accuracy: {rf_accuracy:.2f}')\n",
    "\n",
    "# Feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), np.array(wine.feature_names)[indices], rotation=90)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 8: K-Nearest Neighbors (KNN)\n",
    "**Objective**: Implement KNN for classification.\n",
    "**Dataset**: Iris\n",
    "**Steps**:\n",
    "1. Train KNN model.\n",
    "2. Experiment with different k values.\n",
    "3. Evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Experiment with different k values\n",
    "k_values = range(1, 21)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Plot accuracies\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.title('KNN Accuracy vs. k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Train final model with best k\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Best k: {best_k}, Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 9: Support Vector Machines (SVM)\n",
    "**Objective**: Use SVM for classification.\n",
    "**Dataset**: Iris\n",
    "**Steps**:\n",
    "1. Train SVM with different kernels.\n",
    "2. Evaluate performance.\n",
    "3. Visualize decision boundaries (for 2D data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset (use 2 features for visualization)\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # First two features\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM with different kernels\n",
    "kernels = ['linear', 'rbf']\n",
    "for kernel in kernels:\n",
    "    model = SVC(kernel=kernel, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'{kernel} kernel Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "\n",
    "# Visualize decision boundaries (linear kernel)\n",
    "model = SVC(kernel='linear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "plt.title('SVM Decision Boundaries (Linear Kernel)')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 10: K-Means Clustering\n",
    "**Objective**: Perform clustering on customer data.\n",
    "**Dataset**: [Mall Customers](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)\n",
    "**Steps**:\n",
    "1. Load dataset and select features.\n",
    "2. Apply K-Means clustering.\n",
    "3. Visualize clusters and evaluate with silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate Mall Customers dataset\n",
    "np.random.seed(42)\n",
    "X = np.vstack([\n",
    "    np.random.normal([30, 30000], [5, 5000], (50, 2)),\n",
    "    np.random.normal([40, 60000], [5, 10000], (50, 2)),\n",
    "    np.random.normal([50, 90000], [5, 15000], (50, 2))\n",
    "])\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Evaluate with silhouette score\n",
    "silhouette = silhouette_score(X, labels)\n",
    "print(f'Silhouette Score: {silhouette:.2f}')\n",
    "\n",
    "# Visualize clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Annual Income')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 11: Principal Component Analysis (PCA)\n",
    "**Objective**: Reduce dimensionality with PCA.\n",
    "**Dataset**: Iris\n",
    "**Steps**:\n",
    "1. Apply PCA to reduce dimensions.\n",
    "2. Visualize explained variance.\n",
    "3. Train a classifier on reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Visualize explained variance\n",
    "print(f'Explained Variance Ratio: {pca.explained_variance_ratio_}')\n",
    "plt.bar(range(1, 3), pca.explained_variance_ratio_, tick_label=['PC1', 'PC2'])\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Train classifier on reduced data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy on PCA data: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 12: Gradient Boosting with XGBoost\n",
    "**Objective**: Use XGBoost for classification.\n",
    "**Dataset**: Titanic\n",
    "**Steps**:\n",
    "1. Train XGBoost model.\n",
    "2. Evaluate performance.\n",
    "3. Analyze feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate Titanic dataset\n",
    "data = pd.DataFrame({\n",
    "    'Pclass': [1, 2, 3, 1, 3],\n",
    "    'Age': [38, 26, 22, 35, 28],\n",
    "    'Fare': [71.83, 13.0, 7.25, 53.1, 8.05],\n",
    "    'Survived': [1, 1, 0, 1, 0]\n",
    "})\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data['Survived']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost\n",
    "model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "\n",
    "# Feature importance\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 13: Hyperparameter Tuning\n",
    "**Objective**: Tune model hyperparameters.\n",
    "**Dataset**: Any from previous days\n",
    "**Steps**:\n",
    "1. Use GridSearchCV to tune parameters.\n",
    "2. Compare performance before and after tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train default model\n",
    "default_model = RandomForestClassifier(random_state=42)\n",
    "default_model.fit(X_train, y_train)\n",
    "default_pred = default_model.predict(X_test)\n",
    "default_accuracy = accuracy_score(y_test, default_pred)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Run GridSearchCV\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tuned model\n",
    "tuned_model = grid_search.best_estimator_\n",
    "tuned_pred = tuned_model.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, tuned_pred)\n",
    "\n",
    "print(f'Default Accuracy: {default_accuracy:.2f}')\n",
    "print(f'Tuned Accuracy: {tuned_accuracy:.2f}')\n",
    "print(f'Best Parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 14: Cross-Validation\n",
    "**Objective**: Implement cross-validation for robust evaluation.\n",
    "**Dataset**: Any from previous days\n",
    "**Steps**:\n",
    "1. Perform k-fold cross-validation.\n",
    "2. Compare with train-test split results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(random_state=42, max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "split_pred = model.predict(X_test)\n",
    "split_accuracy = accuracy_score(y_test, split_pred)\n",
    "\n",
    "# K-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f'Train-Test Split Accuracy: {split_accuracy:.2f}')\n",
    "print(f'Cross-Validation Accuracy: {cv_scores.mean():.2f} Â± {cv_scores.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 15: Confusion Matrix and Metrics\n",
    "**Objective**: Evaluate classification models.\n",
    "**Dataset**: Any classification dataset\n",
    "**Steps**:\n",
    "1. Generate confusion matrix.\n",
    "2. Calculate precision, recall, F1-score.\n",
    "3. Visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(random_state=42, max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 16: Time Series Forecasting\n",
    "**Objective**: Forecast time series data with ARIMA.\n",
    "**Dataset**: [Air Passengers](https://www.kaggle.com/rakannimer/air-passengers)\n",
    "**Steps**:\n",
    "1. Load and preprocess time series data.\n",
    "2. Fit ARIMA model.\n",
    "3. Forecast and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate Air Passengers dataset\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='1949-01-01', periods=144, freq='M')\n",
    "data = pd.Series(np.cumsum(np.random.randn(144)) + 100, index=dates)\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(data, order=(5, 1, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=12)\n",
    "\n",
    "# Visualize\n",
    "plt.plot(data, label='Historical')\n",
    "plt.plot(forecast, label='Forecast', color='red')\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 17: Introduction to Neural Networks\n",
    "**Objective**: Learn TensorFlow/Keras basics.\n",
    "**Dataset**: Iris\n",
    "**Steps**:\n",
    "1. Build a simple neural network.\n",
    "2. Train and evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y = to_categorical(y)  # One-hot encode labels\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build neural network\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(4,)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 18: Feedforward Neural Network\n",
    "**Objective**: Build a deeper neural network.\n",
    "**Dataset**: Any classification dataset\n",
    "**Steps**:\n",
    "1. Design a multi-layer network.\n",
    "2. Train and evaluate.\n",
    "3. Visualize loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Design deeper network\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(4,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Visualize loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 19: Convolutional Neural Networks (CNNs)\n",
    "**Objective**: Build a CNN for image classification.\n",
    "**Dataset**: [MNIST](https://www.tensorflow.org/datasets/catalog/mnist)\n",
    "**Steps**:\n",
    "1. Load and preprocess images.\n",
    "2. Build and train CNN.\n",
    "3. Evaluate and visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Build CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Visualize predictions\n",
    "predictions = model.predict(X_test[:5])\n",
    "for i in range(5):\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Predicted: {predictions[i].argmax()}, Actual: {y_test[i].argmax()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 20: Transfer Learning\n",
    "**Objective**: Use pre-trained models for classification.\n",
    "**Dataset**: [Cats vs Dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs)\n",
    "**Steps**:\n",
    "1. Load pre-trained model (e.g., VGG16).\n",
    "2. Fine-tune on dataset.\n",
    "3. Evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Simulate Cats vs Dogs dataset (small sample for demo)\n",
    "X = np.random.rand(100, 224, 224, 3)  # Simulated images\n",
    "y = np.random.randint(0, 2, 100)  # Binary labels\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Load pre-trained VGG16\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=3, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 21: NLP - Text Preprocessing\n",
    "**Objective**: Preprocess text data for NLP.\n",
    "**Dataset**: Any text dataset (e.g., IMDB reviews)\n",
    "**Steps**:\n",
    "1. Tokenize and clean text.\n",
    "2. Remove stop words.\n",
    "3. Create word frequency plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text (simulating IMDB reviews)\n",
    "text = \"This movie is great! I love the acting and the story. The movie was amazing.\"\n",
    "\n",
    "# Tokenize and clean\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Word frequency\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "# Plot\n",
    "plt.bar(word_freq.keys(), word_freq.values())\n",
    "plt.title('Word Frequency')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 22: Sentiment Analysis\n",
    "**Objective**: Build a sentiment analysis model.\n",
    "**Dataset**: [IMDB](https://www.tensorflow.org/datasets/catalog/imdb_reviews)\n",
    "**Steps**:\n",
    "1. Preprocess text and create Bag of Words.\n",
    "2. Train logistic regression.\n",
    "3. Evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simulate IMDB dataset\n",
    "texts = [\n",
    "    \"This movie is great and amazing\",\n",
    "    \"Terrible film, really bad\",\n",
    "    \"I loved the story\",\n",
    "    \"Not good, very boring\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Preprocess text (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 23: Word Embeddings\n",
    "**Objective**: Use Word2Vec for embeddings.\n",
    "**Dataset**: Any text corpus\n",
    "**Steps**:\n",
    "1. Train Word2Vec model.\n",
    "2. Visualize embeddings with t-SNE.\n",
    "3. Find similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "    ['deep', 'learning', 'is', 'powerful'],\n",
    "    ['artificial', 'intelligence', 'is', 'exciting']\n",
    "]\n",
    "\n",
    "# Train Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Find similar words\n",
    "print(\"Words similar to 'learning':\", model.wv.most_similar('learning', topn=3))\n",
    "\n",
    "# Visualize embeddings with t-SNE\n",
    "words = list(model.wv.key_to_index)\n",
    "vectors = model.wv[words]\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "vectors_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))\n",
    "plt.title('Word Embeddings (t-SNE)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 24: Text Classification with LSTM\n",
    "**Objective**: Build an LSTM for text classification.\n",
    "**Dataset**: IMDB\n",
    "**Steps**:\n",
    "1. Preprocess text and create sequences.\n",
    "2. Build and train LSTM.\n",
    "3. Evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simulate IMDB dataset\n",
    "texts = [\n",
    "    \"This movie is great and amazing\",\n",
    "    \"Terrible film, really bad\",\n",
    "    \"I loved the story\",\n",
    "    \"Not good, very boring\"\n",
    "] * 25  # Increase size\n",
    "labels = [1, 0, 1, 0] * 25\n",
    "\n",
    "# Preprocess text\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32, input_length=10),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=16, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 25: Reinforcement Learning\n",
    "**Objective**: Implement Q-Learning basics.\n",
    "**Environment**: [OpenAI Gym](https://gym.openai.com/)\n",
    "**Steps**:\n",
    "1. Set up a simple environment (e.g., FrozenLake).\n",
    "2. Implement Q-Learning algorithm.\n",
    "3. Visualize learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Q-Learning parameters\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Visualize rewards\n",
    "plt.plot(rewards)\n",
    "plt.title('Q-Learning Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 26: Simple Chatbot\n",
    "**Objective**: Build a chatbot with NLTK or Rasa.\n",
    "**Resources**: [NLTK Docs](https://www.nltk.org/), [Rasa Docs](https://rasa.com/docs/)\n",
    "**Steps**:\n",
    "1. Create a rule-based or ML-based chatbot.\n",
    "2. Test with sample inputs.\n",
    "3. Save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# Define chatbot pairs\n",
    "pairs = [\n",
    "    [r'hi|hello', ['Hello!', 'Hi there!']],\n",
    "    [r'how are you', ['I am doing great, thanks!']],\n",
    "    [r'what is your name', ['I am Grok, your friendly chatbot.']],\n",
    "    [r'quit', ['Bye!']]\n",
    "]\n",
    "\n",
    "# Create chatbot\n",
    "chatbot = Chat(pairs, reflections)\n",
    "\n",
    "# Test chatbot\n",
    "print(\"Type 'quit' to exit\")\n",
    "chatbot.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 27: Model Deployment\n",
    "**Objective**: Deploy a model with Flask.\n",
    "**Dataset**: Any classification model\n",
    "**Steps**:\n",
    "1. Train a model and save it.\n",
    "2. Create a Flask app for predictions.\n",
    "3. Test API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Train and save model\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "model = LogisticRegression(random_state=42, max_iter=200)\n",
    "model.fit(X, y)\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "# Create Flask app\n",
    "app = Flask(__name__)\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    features = [data['features']]\n",
    "    prediction = model.predict(features)\n",
    "    return jsonify({'prediction': int(prediction[0])})\n",
    "\n",
    "# Test API (run in a separate script)\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n",
    "\n",
    "# Test with curl or requests:\n",
    "# curl -X POST -H \"Content-Type: application/json\" -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}' http://127.0.0.1:5000/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 28: Model Monitoring\n",
    "**Objective**: Monitor model performance and drift.\n",
    "**Dataset**: Any previous dataset\n",
    "**Steps**:\n",
    "1. Simulate new data.\n",
    "2. Check for data drift.\n",
    "3. Retrain model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train initial model\n",
    "model = LogisticRegression(random_state=42, max_iter=200)\n",
    "model.fit(X, y)\n",
    "initial_pred = model.predict(X)\n",
    "initial_accuracy = accuracy_score(y, initial_pred)\n",
    "\n",
    "# Simulate new data (with drift)\n",
    "X_new = X + np.random.normal(0, 0.5, X.shape)  # Add noise\n",
    "new_pred = model.predict(X_new)\n",
    "new_accuracy = accuracy_score(y, new_pred)\n",
    "\n",
    "# Check for drift\n",
    "print(f'Initial Accuracy: {initial_accuracy:.2f}')\n",
    "print(f'New Data Accuracy: {new_accuracy:.2f}')\n",
    "\n",
    "# Retrain if drift detected\n",
    "if new_accuracy < initial_accuracy * 0.9:\n",
    "    model.fit(X_new, y)\n",
    "    retrained_pred = model.predict(X_new)\n",
    "    print(f'Retrained Accuracy: {accuracy_score(y, retrained_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 29: Capstone Project\n",
    "**Objective**: Build an end-to-end ML pipeline.\n",
    "**Dataset**: [Stock Prices](https://www.kaggle.com/datasets/timoboz/stock-prices)\n",
    "**Steps**:\n",
    "1. Preprocess data and engineer features.\n",
    "2. Train a model (e.g., LSTM or regression).\n",
    "3. Evaluate and visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate stock prices\n",
    "np.random.seed(42)\n",
    "prices = np.cumsum(np.random.randn(200)) + 100\n",
    "\n",
    "# Preprocess data\n",
    "scaler = MinMaxScaler()\n",
    "prices_scaled = scaler.fit_transform(prices.reshape(-1, 1))\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(prices_scaled, 10)\n",
    "X_train, X_test = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
    "y_train, y_test = y[:int(0.8*len(y))], y[int(0.8*len(y)):]\n",
    "\n",
    "# Build LSTM\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(10, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, verbose=0)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Visualize\n",
    "plt.plot(scaler.inverse_transform(y_test), label='Actual')\n",
    "plt.plot(scaler.inverse_transform(y_pred), label='Predicted')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 30: Present and Share\n",
    "**Objective**: Share your projects.\n",
    "**Resources**: [GitHub](https://github.com/), [Kaggle](https://www.kaggle.com/)\n",
    "**Steps**:\n",
    "1. Create a GitHub repository for your projects.\n",
    "2. Write a blog post or Kaggle notebook.\n",
    "3. Share on social media or forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for sharing\n",
    "print(\"1. Create a GitHub repository:\\n\"\n",
    "      \"   - Go to github.com and create a new repository.\\n\"\n",
    "      \"   - Push this notebook: `git add .`, `git commit -m 'Add 30 Days of ML'`, `git push`.\\n\")\n",
    "print(\"2. Write a blog post or Kaggle notebook:\\n\"\n",
    "      \"   - Summarize your 30-day journey.\\n\"\n",
    "      \"   - Upload to Kaggle or a blog platform like Medium.\\n\")\n",
    "print(\"3. Share on social media:\\n\"\n",
    "      \"   - Post on LinkedIn or Twitter with #30DaysOfML.\\n\"\n",
    "      \"   - Include your GitHub/Kaggle link.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}